{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.0 follow setup instructions\n",
    "\n",
    "ℹ️ use [`pylcaio.yml`](https://github.com/michaelweinold/config_conda/blob/main/pylcaio.yml) to set up working conda environment.\n",
    "\n",
    "### 0.1. imports\n",
    "#### 0.1.1. regular imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i/o\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import gzip\n",
    "import pickle\n",
    "import git\n",
    "import json\n",
    "# os specific settings\n",
    "import platform\n",
    "# configuration\n",
    "import yaml\n",
    "# lca\n",
    "import ecospold2matrix as e2m\n",
    "import pymrio\n",
    "#import brightway2 as bw\n",
    "# type hints\n",
    "from ecospold2matrix import ecospold2matrix\n",
    "from pymrio import IOSystem\n",
    "# data science\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# deep copy\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.1.2. load configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../config.yaml', 'r') as filestream:\n",
    "    config = yaml.load(filestream, Loader = yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.1.3. load `pylcaio`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(Path.home(), config['pylcaio'])) # required for local import of pylcaio\n",
    "import pylcaio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2. file paths\n",
    "#### 0.2.1. directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# home directory\n",
    "print(path_dir_home := Path.home())\n",
    "print(path_dir_repo := git.Repo('.', search_parent_directories=True).working_tree_dir)\n",
    "# input directory\n",
    "print(path_dir_databases := os.path.join(path_dir_home, config['path_dir_databases']))\n",
    "# output directories\n",
    "print(path_dir_data := os.path.join(path_dir_home, config['path_dir_data']))\n",
    "print(path_dir_pylcaio := os.path.join(path_dir_home, path_dir_data, config['path_dir_pylcaio']))\n",
    "print(path_dir_pymrio := os.path.join(path_dir_home, path_dir_data, config['path_dir_pymrio']))\n",
    "print(path_dir_e2m := os.path.join(path_dir_home, path_dir_data, config['path_dir_e2m']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.2.2. files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# databases\n",
    "print(path_exiobase := os.path.join(path_dir_home, path_dir_databases, config['exiobase']))\n",
    "print(path_dir_ecoinvent := os.path.join(path_dir_home, path_dir_databases, config['ecoinvent']))\n",
    "# pylcaio output\n",
    "print(path_pylcaio_database_loader_class_instance := os.path.join(path_dir_pylcaio, config['pylcaio_database_loader_class_instance']))\n",
    "print(path_pylcaio_class_instance_before_hybrid := os.path.join(path_dir_pylcaio, config['pylcaio_class_instance_before_hybrid']))\n",
    "print(path_pylcaio_class_instance_after_hybrid := os.path.join(path_dir_pylcaio, config['pylcaio_class_instance_after_hybrid']))\n",
    "# pymrio output\n",
    "print(path_pymrio_class_instance := os.path.join(path_dir_pymrio, config['pymrio_class_instance']))\n",
    "# e2m output\n",
    "print(e2m_project_name := config['e2m_project_name'])\n",
    "print(path_file_e2m_pickle := os.path.join(path_dir_e2m, e2m_project_name + config['e2m_pickle_filename']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "print(path_dict_io_countries_per_lca_region := os.path.join(path_dir_repo, config['path_dict_io_countries_per_lca_region']))\n",
    "print(path_list_io_countries_and_regions := os.path.join(path_dir_repo, config['path_list_io_countries_and_regions']))\n",
    "print(path_list_io_countries := os.path.join(path_dir_repo, config['path_list_io_countries']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file = path_dict_io_countries_per_lca_region, mode = 'r', encoding = 'utf-8') as filestream:\n",
    "    dict_io_countries_per_lca_region: dict = json.load(fp = filestream)\n",
    "with open(file = path_list_io_countries_and_regions, mode = 'r', encoding = 'utf-8') as filestream:\n",
    "    list_io_countries_and_regions: list = json.load(fp = filestream)\n",
    "with open(file = path_list_io_countries, mode = 'r', encoding = 'utf-8') as filestream:\n",
    "    list_io_countries: list = json.load(fp = filestream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRO_f dataframe:\n",
    "\n",
    "| index | activityNameId | io_geography |\n",
    "| ----- | -------------- | ------------ |\n",
    "| 10 | 1 | RoW |\n",
    "| 11 | 1 | CH |\n",
    "| 12 | 1 | AT |\n",
    "| 13 | 2 | RoW |\n",
    "| 14 | 2 | DE |\n",
    "| 15 | 2 | CH |\n",
    "| 16 | 3 | FR |\n",
    "| 17 | 3 | BE |\n",
    "| 18 | 4 | RoW |\n",
    "| 19 | 4 | CH |\n",
    "| 20 | 4 | AT |\n",
    "\n",
    "should look like:\n",
    "\n",
    "| activityNameId | io_geography_list | RoW_region |\n",
    "| -------------- | ----------------- | ---------- |\n",
    "| 1 | RoW, CH, AT | RoW(1) |\n",
    "| 2 | RoW, DE, CH | RoW(2) |\n",
    "| 4 | RoW, CH, AT | RoW(1) |\n",
    "\n",
    "where for activityNameId == 1, RoW region is list_io_countries - [CH, AT]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_pylcaio_class_instance_before_hybrid, 'rb') as file_in:\n",
    "    pylcaio_object_before_hybrid: pylcaio.LCAIO = pd.read_pickle(file_in)\n",
    "PRO_f = pylcaio_object_before_hybrid.PRO_f\n",
    "df = PRO_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. function implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check: https://stackoverflow.com/a/53343046"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_rest_of_world_regions(\n",
    "    df: pd.DataFrame,\n",
    "    list_io_countries: list,\n",
    "    dict_io_countries_per_lca_region: dict\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    # get array of those master activities (key 'activityNameId') where one defined geography is 'RoW'\n",
    "    master_activities_with_rest_of_world_geography: np.ndarray = df[df['io_geography'] == 'RoW']['activityNameId'].unique()\n",
    "    # replace LCA regions (eg. 'Europe without Switzerland') with the associated IO countries (eg. '\"AT\", \"BE\", \"BG\", ...')\n",
    "    df['io_geography'] = df['io_geography'].map(dict_io_countries_per_lca_region).fillna(df['io_geography'])\n",
    "    # remove all activities whose associated master activities where no defined geography is 'RoW'\n",
    "    df = df[df['activityNameId'].isin(master_activities_with_rest_of_world_geography)]\n",
    "\n",
    "    df_with_geography_rest_of_world = df[df['io_geography'] == 'RoW']['activityNameId']\n",
    "    \n",
    "    # remove all activities where 'io_geography' == 'RoW'\n",
    "    df_aggregated = df.drop(master_activities_with_rest_of_world_geography.index)\n",
    "    # group by master activities (key 'activityNameId')\n",
    "    df_aggregated = pd.DataFrame(data = df_aggregated.groupby('activityNameId')['io_geography'].apply(tuple))\n",
    "    # XXXXX\n",
    "    df_aggregated = df_aggregated.merge(\n",
    "        right = df[df.index.isin(master_activities_with_rest_of_world_geography.index)][['activityNameId', ]],\n",
    "        how = 'left',\n",
    "        on = 'activityNameId'\n",
    "    )\n",
    "\n",
    "    # calculate 'rest of world' region per activity\n",
    "    df_aggregated['io_geography_rest_of_world'] = df_aggregated.apply(\n",
    "        lambda row: tuple(set(list_io_countries) - set(row['io_geography'])),\n",
    "        axis = 1\n",
    "    )\n",
    "    # remove activities where 'rest of world' region is null because all countries are described\n",
    "    df_aggregated = df_aggregated.dropna(subset ='io_geography_rest_of_world')\n",
    "\n",
    "    # get unique rest of world regions and label them (eg. [AT, CH, DE]: 'RoW(1)')\n",
    "    unique_rest_of_world_regions = pd.DataFrame(\n",
    "        data = df_aggregated['io_geography_rest_of_world'].unique(),\n",
    "        columns = ['io_geography_rest_of_world'])\n",
    "    unique_rest_of_world_regions['io_geography_rest_of_world_index'] = ['RoW({})'.format(i) for i in unique_rest_of_world_regions.index]\n",
    "\n",
    "    # add 'io_geography_rest_of_world_index' information to aggregated dataframe\n",
    "    df_aggregated = df_aggregated.merge(\n",
    "        right = unique_rest_of_world_regions,\n",
    "        how = 'left',\n",
    "        on = 'io_geography_rest_of_world'\n",
    "    )\n",
    "\n",
    "    # update original dataframe:\n",
    "    df = df.update(\n",
    "        other = df_aggregated,\n",
    "        join = 'left',\n",
    "        overwrite = True\n",
    "    )\n",
    "\n",
    "    # add all rest of world information to original dataframe\n",
    "    df_activities_with_rest_of_world_geography = df_activities_with_rest_of_world_geography.merge(\n",
    "        right = df_aggregated[['activityNameId', 'io_geography_rest_of_world_index']],\n",
    "        how = 'left',\n",
    "        on = 'activityNameId'\n",
    "    )\n",
    "    #df_activities_with_rest_of_world_geography.rename(columns={\"io_geography_rest_of_world_index\": \"io_geography})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_test \u001b[38;5;241m=\u001b[39m \u001b[43midentify_rest_of_world_regions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlist_io_countries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlist_io_countries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdict_io_countries_per_lca_region\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdict_io_countries_per_lca_region\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [25], line 17\u001b[0m, in \u001b[0;36midentify_rest_of_world_regions\u001b[0;34m(df, list_io_countries, dict_io_countries_per_lca_region)\u001b[0m\n\u001b[1;32m     14\u001b[0m df_with_geography_rest_of_world \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mio_geography\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRoW\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivityNameId\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# remove all activities where 'io_geography' == 'RoW'\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m df_aggregated \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(\u001b[43mmaster_activities_with_rest_of_world_geography\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# group by master activities (key 'activityNameId')\u001b[39;00m\n\u001b[1;32m     19\u001b[0m df_aggregated \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data \u001b[38;5;241m=\u001b[39m df_aggregated\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivityNameId\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mio_geography\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mtuple\u001b[39m))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "df_test = identify_rest_of_world_regions(df = df, list_io_countries=list_io_countries, dict_io_countries_per_lca_region=dict_io_countries_per_lca_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_rows(self):\n",
    "    \"\"\" Method to identify the various unique Rest of the World (RoW) regions of the LCA database\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "        The updated self.dictRoW in which unique RoW regions are identified in terms of countries they include\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # contains a list of activityNameId's that are 'PRO_f.io_geography[i]=='RoW (=LCA RoW)'\n",
    "    unique_activities_using_row = list(\n",
    "        set( # removes duplicates\n",
    "            self.PRO_f.activityNameId[ # nota bene: activityNameId is Master Data!\n",
    "                [i for i in self.PRO_f.index if self.PRO_f.io_geography[i] == 'RoW']].tolist()\n",
    "        )\n",
    "    )\n",
    "\n",
    "    RoW_activities = defaultdict(list)\n",
    "\n",
    "    # how and WHERE are `geography` and `io_geography` matched?\n",
    "\n",
    "    # list of tuples (activityNameId (=Master Data), geography)\n",
    "    # where activityNameId where one of the geographis is 'RoW' and\n",
    "    # where geography is a list of all regions that are associated with one activityNameId\n",
    "    tupl = [  \n",
    "        i for i in zip( # zip creates tuple\n",
    "            self.PRO_f.activityNameId.loc[\n",
    "                [\n",
    "                    i for i in self.PRO_f.index\n",
    "                    if self.PRO_f.activityNameId[i] in unique_activities_using_row\n",
    "                ]\n",
    "            ],\n",
    "            self.PRO_f.io_geography.loc[\n",
    "                [\n",
    "                    i for i in self.PRO_f.index\n",
    "                    if self.PRO_f.activityNameId[i] in unique_activities_using_row\n",
    "                ]\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # ok, dictionary is filled\n",
    "    for activity, geography in tupl:\n",
    "        RoW_activities[activity].append(geography)\n",
    "\n",
    "    # remove 'RoW' from dict values\n",
    "    RoW_activities = {activity: [geography1 for geography1 in geography if geography1 != 'RoW'] for activity, geography in RoW_activities.items()}\n",
    "\n",
    "    # delete from RoW_activities processes which had only RoW as geography and are thus empty now\n",
    "    for key in [i for i in list(RoW_activities.keys()) if RoW_activities[i] == []]:\n",
    "        del RoW_activities[key]\n",
    "        \n",
    "    # put every element to the same level (elements that are lists are transformed to lists of lists)\n",
    "    for values in list(RoW_activities.values()):\n",
    "        for i in range(0, len(values)):\n",
    "            if values[i] in self.countries_per_regions.keys():\n",
    "                values[i] = self.countries_per_regions[values[i]]\n",
    "\n",
    "    # for elements that are lists of lists stemming from the replacement of ['RER'] by [['AT','BE',...]],\n",
    "    # add all of the together in a single list\n",
    "    for keys in RoW_activities.keys():\n",
    "        for item in RoW_activities[keys]:\n",
    "            if isinstance(item, list):\n",
    "                RoW_activities[keys] = sum_elements_list(RoW_activities[keys])\n",
    "    # remove duplicates inside the elements\n",
    "    for keys in list(RoW_activities.keys()):\n",
    "        RoW_activities[keys] = list(set(RoW_activities[keys]))\n",
    "\n",
    "    # why sort here? to ensure unique_RoWs does not contain duplicates that would just have a different order\n",
    "    # need to sort to identify duplicates whose elements would be ordered differently and thus be treated as not duplicated\n",
    "    for keys in RoW_activities.keys():\n",
    "        RoW_activities[keys].sort()\n",
    "\n",
    "    # identify the combination of countries that are NOT inside the residual of each process\n",
    "    # dict where\n",
    "    # key = activityNameId\n",
    "    # value = list of IO countries that are NOT associated with activityNameId\n",
    "    dictactrow = {}\n",
    "    residual_geo_IO_to_remove = ['WA', 'WE', 'WF', 'WL', 'WM'] # exiobase 'rest of world' regions (all other are actual countries)\n",
    "    for keys in RoW_activities.keys():\n",
    "        dictactrow[keys] = list( # dictactrow[keys] is activityNameId\n",
    "            set(self.listcountry) # countries + RoW regions IO\n",
    "            - set(RoW_activities[keys]) # countries LCA associated with the activityNameId\n",
    "            - set(residual_geo_IO_to_remove)) # RoW regions IO\n",
    "    \n",
    "    unique_RoWs = []\n",
    "    for keys in dictactrow.keys():\n",
    "        if dictactrow[keys] not in unique_RoWs:\n",
    "            unique_RoWs.append(dictactrow[keys])\n",
    "\n",
    "    # create name for the values of the different RoW\n",
    "    listname = []\n",
    "    for i in range(0, len(unique_RoWs)):\n",
    "        listname.append('RoW' + '(' + str(i) + ')')\n",
    "    \n",
    "    # put all of that in dictRoW\n",
    "    # dictRow is created empty earlier in the code\n",
    "    for i in range(0, len(unique_RoWs)):\n",
    "        self.dictRoW[listname[i]] = unique_RoWs[i]\n",
    "    try:\n",
    "        # if RoWs are empty because processes from ecoinvent are too described\n",
    "        del [[k for k in self.dictRoW.keys() if len(self.dictRoW[k]) == 0][0]]\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "    # replace RoW list (eg. [AT, DE, ...] with RoW number (eg. RoW(1)))\n",
    "    for activityNameId in dictactrow: # key = activityNameId\n",
    "        for RoW_number in self.dictRoW: # key = eg. \"RoW(12)\"\n",
    "            if dictactrow[activityNameId] == self.dictRoW[RoW_number]: # dictactrow[activityNameId] = RoW list, self.dictRoW[RoW_number]\n",
    "                dictactrow[activityNameId] = RoW_number\n",
    "\n",
    "    RoW_matrix = pd.DataFrame(\n",
    "        data = list(dictactrow.values()),\n",
    "        index=list(dictactrow.keys()),\n",
    "        columns=['RoW_geography']\n",
    "    )\n",
    "\n",
    "    # adds RoW information to matrix\n",
    "    self.PRO_f = self.PRO_f.merge(RoW_matrix, left_on='activityNameId', right_on=RoW_matrix.index, how='outer')\n",
    "\n",
    "    self.PRO_f.index = self.PRO_f.activityId + '_' + self.PRO_f.productId\n",
    "    self.PRO_f = self.PRO_f.reindex(self.processes_in_order)\n",
    "    \n",
    "    self.PRO_f.io_geography.update(self.PRO_f.RoW_geography[self.PRO_f.io_geography == 'RoW'])\n",
    "    self.PRO_f = self.PRO_f.drop('RoW_geography', axis=1)\n",
    "\n",
    "    # might be some RoW or empty lists left in PRO_f\n",
    "    self.PRO_f.io_geography[self.PRO_f.io_geography == 'RoW'] = 'GLO'\n",
    "    self.PRO_f.io_geography.loc[[i for i in self.PRO_f.index if type(self.PRO_f.io_geography[i]) == list]] = 'GLO'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('pylcaio')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af11b17298303d3420ac04a2d81997bb73d15b4df2e0afffe06f0aa93756cff5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
